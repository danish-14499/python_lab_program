{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e0a051-c0c2-4d56-8e36-ea6c70c73340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequent Itemsets:\n",
      "Itemset: ('Star Wars',), Support: 0.60\n",
      "Itemset: ('DC Comics',), Support: 0.60\n",
      "Itemset: ('Marvel',), Support: 0.50\n",
      "Itemset: ('Star Trek',), Support: 0.40\n",
      "Itemset: ('Marvel', 'DC Comics'), Support: 0.30\n",
      "Itemset: ('Marvel', 'Star Wars'), Support: 0.30\n",
      "Itemset: ('Star Wars', 'Star Trek'), Support: 0.30\n",
      "\n",
      "Association Rules:\n",
      "Rule: {'Marvel'} -> {'DC Comics'}, Confidence: 0.60, Lift: 10.00\n",
      "Rule: {'Marvel'} -> {'Star Wars'}, Confidence: 0.60, Lift: 10.00\n",
      "Rule: {'Star Trek'} -> {'Star Wars'}, Confidence: 0.75, Lift: 12.50\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Sample dataset\n",
    "transactions = [\n",
    "    [\"Star Wars\", \"Marvel\", \"DC Comics\"],\n",
    "    [\"Star Wars\", \"Star Trek\"],\n",
    "    [\"Marvel\", \"DC Comics\"],\n",
    "    [\"Star Wars\", \"Marvel\"],\n",
    "    [\"Star Trek\", \"DC Comics\"],\n",
    "    [\"Star Wars\", \"DC Comics\"],\n",
    "    [\"Star Trek\", \"Star Wars\"],\n",
    "    [\"DC Comics\", \"Marvel\"],\n",
    "    [\"Star Wars\", \"Star Trek\", \"Marvel\"],\n",
    "    [\"DC Comics\"]\n",
    "]\n",
    "\n",
    "\n",
    "# Function to generate candidate itemsets of length k\n",
    "def generate_candidates(itemsets, length):\n",
    "    return set(itertools.combinations(itemsets, length))\n",
    "\n",
    "# Function to filter out itemsets that meet the minimum support threshold\n",
    "def filter_frequent_itemsets(transactions, candidates, min_support):\n",
    "    # Step 1: Create a dictionary to count occurrences of each candidate itemset\n",
    "    itemset_count = {}\n",
    "    \n",
    "    # Step 2: Loop through each transaction in the transactions list\n",
    "    for transaction in transactions:\n",
    "        # Step 3: Check each candidate itemset\n",
    "        for candidate in candidates:\n",
    "            # Step 4: If the candidate is part of the transaction, increase its count\n",
    "            if set(candidate).issubset(transaction):\n",
    "                if candidate in itemset_count:\n",
    "                    itemset_count[candidate] += 1\n",
    "                else:\n",
    "                    itemset_count[candidate] = 1\n",
    "\n",
    "    # Step 5: Calculate the total number of transactions\n",
    "    num_transactions = len(transactions)\n",
    "    \n",
    "    # Step 6: Create a dictionary for itemsets that meet the minimum support\n",
    "    frequent_itemsets = {}\n",
    "    for itemset, count in itemset_count.items():\n",
    "        support = count / num_transactions\n",
    "        if support >= min_support:\n",
    "            frequent_itemsets[itemset] = support\n",
    "    \n",
    "    # Step 7: Return the frequent itemsets\n",
    "    return frequent_itemsets\n",
    "\n",
    "\n",
    "# Function to generate association rules from frequent itemsets\n",
    "def generate_association_rules(frequent_itemsets, min_confidence):\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "        for i in range(1, len(itemset)):\n",
    "            for subset in itertools.combinations(itemset, i):\n",
    "                antecedent = set(subset)\n",
    "                consequent = set(itemset) - antecedent\n",
    "                if consequent:\n",
    "                    support_antecedent = frequent_itemsets.get(tuple(sorted(antecedent)), 0)\n",
    "                    support_itemset = frequent_itemsets[itemset]\n",
    "                    confidence = support_itemset / support_antecedent if support_antecedent > 0 else 0\n",
    "                    if confidence >= min_confidence:\n",
    "                        lift = confidence / (frequent_itemsets.get(tuple(sorted(consequent)), 0) / len(transactions))\n",
    "                        rules.append((antecedent, consequent, confidence, lift))\n",
    "    return rules\n",
    "\n",
    "# Step 1: Generate all itemsets\n",
    "itemsets = set(item for transaction in transactions for item in transaction)\n",
    "\n",
    "# Step 2: Initialize\n",
    "min_support = 0.3\n",
    "min_confidence = 0.6\n",
    "frequent_itemsets = {}\n",
    "k = 1\n",
    "\n",
    "# Step 3: Generate frequent itemsets\n",
    "while True:\n",
    "    candidates = generate_candidates(itemsets, k)\n",
    "    frequent_itemsets_k = filter_frequent_itemsets(transactions, candidates, min_support)\n",
    "    if not frequent_itemsets_k:\n",
    "        break\n",
    "    frequent_itemsets.update(frequent_itemsets_k)\n",
    "    k += 1\n",
    "\n",
    "# Step 4: Generate association rules\n",
    "rules = generate_association_rules(frequent_itemsets, min_confidence)\n",
    "\n",
    "# Output results\n",
    "print(\"Frequent Itemsets:\")\n",
    "for itemset, support in frequent_itemsets.items():\n",
    "    print(f\"Itemset: {itemset}, Support: {support:.2f}\")\n",
    "\n",
    "print(\"\\nAssociation Rules:\")\n",
    "for antecedent, consequent, confidence, lift in rules:\n",
    "    print(f\"Rule: {antecedent} -> {consequent}, Confidence: {confidence:.2f}, Lift: {lift:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2372a8-9565-4b2b-9fd3-7cacac358cc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
